{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JCC2 User Questionnaire - Executive Summary\n",
    "\n",
    "This notebook provides a compressed, high-level view of the JCC2 User Questionnaire data, focusing on key metrics and essential visualizations for decision-making.\n",
    "\n",
    "**Last Updated:** {current_date}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'jcc2_unified_processor'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m sys.path.append(\u001b[33m\"\u001b[39m\u001b[33m../data_analysis\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Import the unified processor\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjcc2_unified_processor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m JCC2DataProcessor\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'jcc2_unified_processor'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# Import the unified processor\n",
    "from jcc2_unified_processor import JCC2DataProcessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'jcc2_unified_processor'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m warnings.filterwarnings(\u001b[33m'\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Import the unified processor\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjcc2_unified_processor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m JCC2DataProcessor\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Configure visualization settings\u001b[39;00m\n\u001b[32m     15\u001b[39m plt.style.use(\u001b[33m'\u001b[39m\u001b[33mseaborn-v0_8-darkgrid\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'jcc2_unified_processor'"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure visualization settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Define consistent color scheme\n",
    "COLORS = {\n",
    "    'primary': '#1f77b4',\n",
    "    'secondary': '#ff7f0e', \n",
    "    'success': '#2ca02c',\n",
    "    'danger': '#d62728',\n",
    "    'warning': '#ff9800',\n",
    "    'info': '#17a2b8'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize processor and load data\n",
    "processor = JCC2DataProcessor()\n",
    "df = processor.load_data('mock_20_jcc2_user_questionnaire.csv')\n",
    "\n",
    "# Update date in header\n",
    "current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "print(f\"Data loaded: {len(df)} responses, {len(df.columns)} fields\")\n",
    "print(f\"Date range: {df['user_information.date'].min()} to {df['user_information.date'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Overall Data Quality & Response Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create overall metrics summary table\n",
    "overall_metrics = []\n",
    "\n",
    "# Basic statistics\n",
    "overall_metrics.append(['Total Responses', len(df)])\n",
    "overall_metrics.append(['Total Fields', len(df.columns)])\n",
    "overall_metrics.append(['Average Completion Rate', f\"{df.notna().sum().sum() / (len(df) * len(df.columns)) * 100:.1f}%\"])\n",
    "overall_metrics.append(['Date Range', f\"{df['user_information.date'].min()} to {df['user_information.date'].max()}\"])\n",
    "\n",
    "# Participation metrics\n",
    "if 'user_information.event' in df.columns:\n",
    "    overall_metrics.append(['Unique Events', df['user_information.event'].nunique()])\n",
    "if 'user_information.unit' in df.columns:\n",
    "    overall_metrics.append(['Unique Units', df['user_information.unit'].nunique()])\n",
    "\n",
    "# Create metrics table\n",
    "metrics_df = pd.DataFrame(overall_metrics, columns=['Metric', 'Value'])\n",
    "print(\"\\n=== OVERALL DATA SUMMARY ===\")\n",
    "print(metrics_df.to_string(index=False))\n",
    "\n",
    "# Quick visualization of response completion by section\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Section completion rates\n",
    "section_completion = {}\n",
    "for section_name, fields in processor.sections.items():\n",
    "    completion_rates = [df[field].notna().mean() for field in fields if field in df.columns]\n",
    "    if completion_rates:\n",
    "        section_completion[section_name] = np.mean(completion_rates)\n",
    "\n",
    "# Top and bottom 5 sections\n",
    "sorted_sections = sorted(section_completion.items(), key=lambda x: x[1], reverse=True)\n",
    "top_sections = sorted_sections[:5]\n",
    "bottom_sections = sorted_sections[-5:]\n",
    "\n",
    "# Plot top sections\n",
    "top_df = pd.DataFrame(top_sections, columns=['Section', 'Completion'])\n",
    "top_df.plot(x='Section', y='Completion', kind='barh', ax=ax1, color=COLORS['success'], legend=False)\n",
    "ax1.set_title('Top 5 Sections by Completion Rate', fontweight='bold')\n",
    "ax1.set_xlabel('Completion Rate')\n",
    "ax1.set_xlim(0, 1)\n",
    "for i, (_, rate) in enumerate(top_sections):\n",
    "    ax1.text(rate + 0.01, i, f'{rate:.1%}', va='center')\n",
    "\n",
    "# Plot bottom sections\n",
    "bottom_df = pd.DataFrame(bottom_sections, columns=['Section', 'Completion'])\n",
    "bottom_df.plot(x='Section', y='Completion', kind='barh', ax=ax2, color=COLORS['warning'], legend=False)\n",
    "ax2.set_title('Bottom 5 Sections by Completion Rate', fontweight='bold')\n",
    "ax2.set_xlabel('Completion Rate')\n",
    "ax2.set_xlim(0, 1)\n",
    "for i, (_, rate) in enumerate(bottom_sections):\n",
    "    ax2.text(rate + 0.01, i, f'{rate:.1%}', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Section-by-Section Key Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define key metrics extractor for each section type\n",
    "def extract_key_metrics(section_name, fields, df):\n",
    "    \"\"\"Extract the most important metrics for a given section.\"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    # Common metrics for all sections\n",
    "    valid_fields = [f for f in fields if f in df.columns]\n",
    "    if valid_fields:\n",
    "        completion_rates = [df[field].notna().mean() for field in valid_fields]\n",
    "        metrics['Fields'] = len(valid_fields)\n",
    "        metrics['Avg Completion'] = f\"{np.mean(completion_rates):.1%}\"\n",
    "    \n",
    "    # Section-specific metrics\n",
    "    if section_name == 'user_information':\n",
    "        if 'user_information.event' in df.columns:\n",
    "            metrics['Events'] = df['user_information.event'].nunique()\n",
    "        if 'user_information.unit' in df.columns:\n",
    "            metrics['Units'] = df['user_information.unit'].nunique()\n",
    "    \n",
    "    elif section_name == 'role_and_echelon':\n",
    "        if 'role_and_echelon.is_cyber_operator' in df.columns:\n",
    "            cyber_pct = (df['role_and_echelon.is_cyber_operator'] == 'Yes').mean() * 100\n",
    "            metrics['Cyber Operators'] = f\"{cyber_pct:.1f}%\"\n",
    "        if 'role_and_echelon.echelon' in df.columns:\n",
    "            top_echelon = df['role_and_echelon.echelon'].value_counts().index[0] if not df['role_and_echelon.echelon'].empty else 'N/A'\n",
    "            metrics['Top Echelon'] = top_echelon\n",
    "    \n",
    "    elif section_name == 'operational_jcc2_experience':\n",
    "        exp_fields = [f for f in fields if 'exp_app_' in f and f in df.columns]\n",
    "        if exp_fields:\n",
    "            total_exp = 0\n",
    "            for field in exp_fields:\n",
    "                total_exp += (df[field] != 'NA').sum()\n",
    "            metrics['App Experience'] = f\"{total_exp / (len(exp_fields) * len(df)) * 100:.1f}%\"\n",
    "    \n",
    "    elif section_name == 'jcc2_application_usage':\n",
    "        freq_fields = [f for f in fields if 'frequency_' in f and f in df.columns]\n",
    "        if freq_fields:\n",
    "            daily_users = sum((df[f] == 'Daily').sum() for f in freq_fields)\n",
    "            metrics['Daily Users'] = daily_users\n",
    "        train_fields = [f for f in fields if 'training_received_' in f and f in df.columns]\n",
    "        if train_fields:\n",
    "            trained = sum((df[f] == 'Yes').sum() for f in train_fields)\n",
    "            total = sum(df[f].notna().sum() for f in train_fields)\n",
    "            metrics['Training Rate'] = f\"{trained/total*100:.1f}%\" if total > 0 else \"0%\"\n",
    "    \n",
    "    elif section_name.startswith('mop_'):\n",
    "        eff_fields = [f for f in fields if 'effectiveness' in f and f in df.columns]\n",
    "        if eff_fields:\n",
    "            effectiveness_map = {'Completely Ineffective': 1, 'Ineffective': 2, 'Somewhat Ineffective': 3,\n",
    "                               'Somewhat Effective': 4, 'Effective': 5, 'Completely Effective': 6}\n",
    "            all_scores = []\n",
    "            for field in eff_fields:\n",
    "                scores = df[field].map(effectiveness_map).dropna()\n",
    "                all_scores.extend(scores.tolist())\n",
    "            if all_scores:\n",
    "                metrics['Avg Effectiveness'] = f\"{np.mean(all_scores):.2f}/6\"\n",
    "                metrics['% Effective'] = f\"{(np.array(all_scores) >= 4).mean() * 100:.1f}%\"\n",
    "    \n",
    "    elif section_name.startswith('mos_'):\n",
    "        suit_fields = [f for f in fields if 'suitability' in f and f in df.columns]\n",
    "        if suit_fields:\n",
    "            suitability_map = {'Completely Unsuitable': 1, 'Unsuitable': 2, 'Somewhat Unsuitable': 3,\n",
    "                             'Somewhat Suitable': 4, 'Suitable': 5, 'Completely Suitable': 6}\n",
    "            all_scores = []\n",
    "            for field in suit_fields:\n",
    "                scores = df[field].map(suitability_map).dropna()\n",
    "                all_scores.extend(scores.tolist())\n",
    "            if all_scores:\n",
    "                metrics['Avg Suitability'] = f\"{np.mean(all_scores):.2f}/6\"\n",
    "                metrics['% Suitable'] = f\"{(np.array(all_scores) >= 4).mean() * 100:.1f}%\"\n",
    "    \n",
    "    elif section_name == 'overall_system_usability':\n",
    "        sus_fields = [f for f in fields if f.startswith('overall_system_usability.sus_') and f in df.columns]\n",
    "        if len(sus_fields) == 10:  # Complete SUS\n",
    "            sus_scores = processor.calculate_sus_scores(df)\n",
    "            if sus_scores:\n",
    "                metrics['Avg SUS Score'] = f\"{np.mean(sus_scores):.1f}\"\n",
    "                metrics['SUS Rating'] = 'Good' if np.mean(sus_scores) >= 68 else 'Poor'\n",
    "    \n",
    "    elif section_name == 'overall_system_suitability_eval':\n",
    "        if 'overall_system_suitability_eval.recommend_jcc2' in df.columns:\n",
    "            rec_counts = df['overall_system_suitability_eval.recommend_jcc2'].value_counts()\n",
    "            yes_pct = rec_counts.get('Yes', 0) / rec_counts.sum() * 100 if rec_counts.sum() > 0 else 0\n",
    "            metrics['Would Recommend'] = f\"{yes_pct:.1f}%\"\n",
    "            \n",
    "    return metrics\n",
    "\n",
    "# Create comprehensive metrics table\n",
    "section_metrics_data = []\n",
    "priority_sections = [\n",
    "    'user_information', 'role_and_echelon', 'operational_jcc2_experience',\n",
    "    'jcc2_application_usage', 'mop_1_1_1', 'mos_1_1_2', 'reporting_and_data_export',\n",
    "    'overall_system_usability', 'overall_system_suitability_eval'\n",
    "]\n",
    "\n",
    "for section in priority_sections:\n",
    "    if section in processor.sections:\n",
    "        metrics = extract_key_metrics(section, processor.sections[section], df)\n",
    "        \n",
    "        # Format section name\n",
    "        display_name = section.replace('_', ' ').title()\n",
    "        if section.startswith('mop_'):\n",
    "            display_name = f\"MOP {section.replace('mop_', '').replace('_', '.')}\"\n",
    "        elif section.startswith('mos_'):\n",
    "            display_name = f\"MOS {section.replace('mos_', '').replace('_', '.')}\"\n",
    "        \n",
    "        # Create row with key metrics\n",
    "        row = [display_name]\n",
    "        for key in ['Fields', 'Avg Completion', 'Avg Effectiveness', 'Avg Suitability', \n",
    "                   'Avg SUS Score', 'Would Recommend', 'Cyber Operators', 'Daily Users']:\n",
    "            row.append(metrics.get(key, '-'))\n",
    "        \n",
    "        section_metrics_data.append(row)\n",
    "\n",
    "# Create DataFrame\n",
    "columns = ['Section', 'Fields', 'Completion', 'Effectiveness', 'Suitability', \n",
    "          'SUS Score', 'Recommend', 'Cyber Ops', 'Daily Users']\n",
    "section_metrics_df = pd.DataFrame(section_metrics_data, columns=columns)\n",
    "\n",
    "print(\"\\n=== SECTION-BY-SECTION KEY METRICS ===\")\n",
    "print(section_metrics_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Critical Insights Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 2x2 dashboard with the most critical insights\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. System Effectiveness Overview (MOP scores)\n",
    "mop_scores = {}\n",
    "effectiveness_map = {'Completely Ineffective': 1, 'Ineffective': 2, 'Somewhat Ineffective': 3,\n",
    "                    'Somewhat Effective': 4, 'Effective': 5, 'Completely Effective': 6}\n",
    "\n",
    "for section in processor.sections:\n",
    "    if section.startswith('mop_'):\n",
    "        eff_fields = [f for f in processor.sections[section] if 'effectiveness' in f and f in df.columns]\n",
    "        if eff_fields:\n",
    "            all_scores = []\n",
    "            for field in eff_fields:\n",
    "                scores = df[field].map(effectiveness_map).dropna()\n",
    "                all_scores.extend(scores.tolist())\n",
    "            if all_scores:\n",
    "                section_label = f\"MOP {section.replace('mop_', '').replace('_', '.')}\"\n",
    "                mop_scores[section_label] = np.mean(all_scores)\n",
    "\n",
    "if mop_scores:\n",
    "    mop_series = pd.Series(mop_scores).sort_values()\n",
    "    mop_series.plot(kind='barh', ax=ax1, color=COLORS['primary'])\n",
    "    ax1.set_title('System Effectiveness by MOP Category', fontweight='bold', fontsize=12)\n",
    "    ax1.set_xlabel('Average Effectiveness Score (1-6)')\n",
    "    ax1.axvline(3.5, color='red', linestyle='--', alpha=0.5, label='Neutral')\n",
    "    ax1.set_xlim(1, 6)\n",
    "    for i, (cat, score) in enumerate(mop_series.items()):\n",
    "        ax1.text(score + 0.05, i, f'{score:.2f}', va='center', fontsize=9)\n",
    "\n",
    "# 2. Application Adoption Rate (Top 10)\n",
    "app_adoption = {}\n",
    "apps = ['a2it', 'cad', 'codex', 'crucible', 'cyber9line', 'dispatch', \n",
    "        'jcc2cyberops', 'jcc2readiness', 'madss', 'rally', 'redmap', \n",
    "        'sigact', 'threathub', 'triage', 'unity']\n",
    "\n",
    "for app in apps:\n",
    "    freq_field = f'jcc2_application_usage.frequency_{app}'\n",
    "    if freq_field in df.columns:\n",
    "        total = df[freq_field].notna().sum()\n",
    "        never = (df[freq_field] == 'Never').sum()\n",
    "        if total > 0:\n",
    "            adoption = (total - never) / total * 100\n",
    "            app_adoption[app.upper()] = adoption\n",
    "\n",
    "if app_adoption:\n",
    "    top_apps = pd.Series(app_adoption).sort_values(ascending=False).head(10)\n",
    "    top_apps.plot(kind='bar', ax=ax2, color=COLORS['success'])\n",
    "    ax2.set_title('Top 10 Applications by Adoption Rate', fontweight='bold', fontsize=12)\n",
    "    ax2.set_ylabel('Adoption Rate (%)')\n",
    "    ax2.set_ylim(0, 100)\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    for i, (app, rate) in enumerate(top_apps.items()):\n",
    "        ax2.text(i, rate + 1, f'{rate:.0f}%', ha='center', fontsize=9)\n",
    "\n",
    "# 3. Overall System Evaluation Metrics\n",
    "eval_metrics = {}\n",
    "\n",
    "# SUS Score\n",
    "sus_fields = [f for f in df.columns if f.startswith('overall_system_usability.sus_')]\n",
    "if len(sus_fields) == 10:\n",
    "    sus_scores = processor.calculate_sus_scores(df)\n",
    "    if sus_scores:\n",
    "        eval_metrics['SUS Score'] = np.mean(sus_scores)\n",
    "\n",
    "# NPS Score\n",
    "nps_scores = processor.calculate_nps_score(df)\n",
    "if nps_scores is not None:\n",
    "    eval_metrics['NPS'] = nps_scores + 50  # Shift to 0-100 scale\n",
    "\n",
    "# Recommendation Rate\n",
    "if 'overall_system_suitability_eval.recommend_jcc2' in df.columns:\n",
    "    rec_counts = df['overall_system_suitability_eval.recommend_jcc2'].value_counts()\n",
    "    if rec_counts.sum() > 0:\n",
    "        eval_metrics['Would Recommend'] = rec_counts.get('Yes', 0) / rec_counts.sum() * 100\n",
    "\n",
    "# Overall Effectiveness\n",
    "if 'reporting_and_data_export.overall_reporting_effectiveness' in df.columns:\n",
    "    eff_counts = df['reporting_and_data_export.overall_reporting_effectiveness'].map(effectiveness_map).dropna()\n",
    "    if len(eff_counts) > 0:\n",
    "        eval_metrics['Reporting Effectiveness'] = (eff_counts >= 4).mean() * 100\n",
    "\n",
    "if eval_metrics:\n",
    "    eval_series = pd.Series(eval_metrics)\n",
    "    colors = [COLORS['success'] if v >= 70 else COLORS['warning'] if v >= 50 else COLORS['danger'] \n",
    "             for v in eval_series.values]\n",
    "    \n",
    "    eval_series.plot(kind='bar', ax=ax3, color=colors)\n",
    "    ax3.set_title('Overall System Evaluation Metrics', fontweight='bold', fontsize=12)\n",
    "    ax3.set_ylabel('Score / Percentage')\n",
    "    ax3.set_ylim(0, 100)\n",
    "    ax3.axhline(70, color='green', linestyle='--', alpha=0.5, label='Good')\n",
    "    ax3.axhline(50, color='orange', linestyle='--', alpha=0.5, label='Acceptable')\n",
    "    ax3.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    for i, (metric, value) in enumerate(eval_series.items()):\n",
    "        ax3.text(i, value + 1, f'{value:.0f}', ha='center', fontsize=9)\n",
    "\n",
    "# 4. Training vs Usage Correlation Summary\n",
    "training_usage_data = []\n",
    "for app in apps[:8]:  # Top 8 apps for clarity\n",
    "    freq_field = f'jcc2_application_usage.frequency_{app}'\n",
    "    train_field = f'jcc2_application_usage.training_received_{app}'\n",
    "    \n",
    "    if freq_field in df.columns and train_field in df.columns:\n",
    "        # Usage intensity (Never=0, Monthly=1, Weekly=2, Daily=3)\n",
    "        usage_map = {'Never': 0, 'Monthly': 1, 'Weekly': 2, 'Daily': 3}\n",
    "        avg_usage = df[freq_field].map(usage_map).mean()\n",
    "        \n",
    "        # Training percentage\n",
    "        train_pct = (df[train_field] == 'Yes').mean() * 100\n",
    "        \n",
    "        if pd.notna(avg_usage) and pd.notna(train_pct):\n",
    "            training_usage_data.append({\n",
    "                'app': app.upper(),\n",
    "                'usage': avg_usage,\n",
    "                'training': train_pct\n",
    "            })\n",
    "\n",
    "if training_usage_data:\n",
    "    tu_df = pd.DataFrame(training_usage_data)\n",
    "    scatter = ax4.scatter(tu_df['training'], tu_df['usage'], \n",
    "                         s=100, alpha=0.6, color=COLORS['info'])\n",
    "    \n",
    "    for _, row in tu_df.iterrows():\n",
    "        ax4.annotate(row['app'], (row['training'], row['usage']), \n",
    "                    xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "    \n",
    "    # Add trend line\n",
    "    z = np.polyfit(tu_df['training'], tu_df['usage'], 1)\n",
    "    p = np.poly1d(z)\n",
    "    ax4.plot(tu_df['training'].sort_values(), p(tu_df['training'].sort_values()), \n",
    "            \"r--\", alpha=0.5, label=f'Trend: {z[0]:.3f}x + {z[1]:.2f}')\n",
    "    \n",
    "    ax4.set_xlabel('Users Trained (%)')\n",
    "    ax4.set_ylabel('Average Usage Intensity (0-3)')\n",
    "    ax4.set_title('Training vs Usage Correlation', fontweight='bold', fontsize=12)\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    ax4.legend()\n",
    "    ax4.set_xlim(-5, 105)\n",
    "    ax4.set_ylim(-0.2, 3.2)\n",
    "\n",
    "plt.suptitle('JCC2 Critical Insights Dashboard', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Executive Summary & Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate executive summary with key findings\n",
    "print(\"\\n=== EXECUTIVE SUMMARY ===\")\n",
    "print(\"\\n1. DATA QUALITY\")\n",
    "print(f\"   - Total responses collected: {len(df)}\")\n",
    "print(f\"   - Overall data completion rate: {df.notna().sum().sum() / (len(df) * len(df.columns)) * 100:.1f}%\")\n",
    "print(f\"   - Sections with >80% completion: {sum(1 for _, rate in section_completion.items() if rate > 0.8)}\")\n",
    "print(f\"   - Sections with <50% completion: {sum(1 for _, rate in section_completion.items() if rate < 0.5)}\")\n",
    "\n",
    "print(\"\\n2. SYSTEM EFFECTIVENESS\")\n",
    "if mop_scores:\n",
    "    avg_mop = np.mean(list(mop_scores.values()))\n",
    "    print(f\"   - Average MOP effectiveness score: {avg_mop:.2f}/6.0\")\n",
    "    print(f\"   - MOP categories rated 'Effective' or better: {sum(1 for score in mop_scores.values() if score >= 4)}/{len(mop_scores)}\")\n",
    "    best_mop = max(mop_scores.items(), key=lambda x: x[1])\n",
    "    worst_mop = min(mop_scores.items(), key=lambda x: x[1])\n",
    "    print(f\"   - Highest performing: {best_mop[0]} ({best_mop[1]:.2f})\")\n",
    "    print(f\"   - Lowest performing: {worst_mop[0]} ({worst_mop[1]:.2f})\")\n",
    "\n",
    "print(\"\\n3. APPLICATION ECOSYSTEM\")\n",
    "if app_adoption:\n",
    "    print(f\"   - Applications tracked: {len(app_adoption)}\")\n",
    "    print(f\"   - Apps with >50% adoption: {sum(1 for rate in app_adoption.values() if rate > 50)}\")\n",
    "    print(f\"   - Average adoption rate: {np.mean(list(app_adoption.values())):.1f}%\")\n",
    "    if top_apps is not None and len(top_apps) > 0:\n",
    "        print(f\"   - Most adopted app: {top_apps.index[0]} ({top_apps.iloc[0]:.0f}%)\")\n",
    "\n",
    "print(\"\\n4. USER SATISFACTION\")\n",
    "if 'SUS Score' in eval_metrics:\n",
    "    sus = eval_metrics['SUS Score']\n",
    "    print(f\"   - System Usability Score (SUS): {sus:.1f}/100\")\n",
    "    print(f\"   - SUS Rating: {'Excellent' if sus >= 80 else 'Good' if sus >= 68 else 'OK' if sus >= 50 else 'Poor'}\")\n",
    "if 'NPS' in eval_metrics:\n",
    "    print(f\"   - Net Promoter Score: {eval_metrics['NPS']-50:.0f}\")\n",
    "if 'Would Recommend' in eval_metrics:\n",
    "    print(f\"   - Would recommend JCC2: {eval_metrics['Would Recommend']:.1f}%\")\n",
    "\n",
    "print(\"\\n5. KEY INSIGHTS\")\n",
    "# Training correlation\n",
    "if training_usage_data and len(tu_df) > 3:\n",
    "    correlation = tu_df['training'].corr(tu_df['usage'])\n",
    "    print(f\"   - Training-Usage Correlation: {correlation:.3f} ({'Positive' if correlation > 0 else 'Negative'})\")\n",
    "\n",
    "# Cyber operator participation\n",
    "if 'role_and_echelon.is_cyber_operator' in df.columns:\n",
    "    cyber_pct = (df['role_and_echelon.is_cyber_operator'] == 'Yes').mean() * 100\n",
    "    print(f\"   - Cyber operator participation: {cyber_pct:.1f}%\")\n",
    "\n",
    "# Response timeline\n",
    "if 'user_information.date' in df.columns:\n",
    "    date_range_days = (pd.to_datetime(df['user_information.date'].max()) - \n",
    "                      pd.to_datetime(df['user_information.date'].min())).days\n",
    "    print(f\"   - Data collection period: {date_range_days} days\")\n",
    "\n",
    "print(\"\\n=== RECOMMENDATIONS ===\")\n",
    "recommendations = []\n",
    "\n",
    "# Based on completion rates\n",
    "low_completion_sections = [s for s, rate in section_completion.items() if rate < 0.5]\n",
    "if low_completion_sections:\n",
    "    recommendations.append(f\"1. Improve data collection for {len(low_completion_sections)} sections with <50% completion\")\n",
    "\n",
    "# Based on effectiveness scores\n",
    "if mop_scores and avg_mop < 4:\n",
    "    recommendations.append(f\"2. Focus improvement efforts on MOP categories scoring below 4.0\")\n",
    "\n",
    "# Based on training\n",
    "if training_usage_data and correlation > 0.3:\n",
    "    low_training_apps = [d['app'] for d in training_usage_data if d['training'] < 30]\n",
    "    if low_training_apps:\n",
    "        recommendations.append(f\"3. Increase training for {', '.join(low_training_apps[:3])} (strong usage correlation)\")\n",
    "\n",
    "# Based on adoption\n",
    "if app_adoption:\n",
    "    low_adoption = [app for app, rate in app_adoption.items() if rate < 20]\n",
    "    if low_adoption:\n",
    "        recommendations.append(f\"4. Review and improve adoption for {len(low_adoption)} applications with <20% usage\")\n",
    "\n",
    "# Based on SUS score\n",
    "if 'SUS Score' in eval_metrics and eval_metrics['SUS Score'] < 68:\n",
    "    recommendations.append(\"5. Prioritize usability improvements to reach 'Good' SUS rating (68+)\")\n",
    "\n",
    "for rec in recommendations:\n",
    "    print(f\"   {rec}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Export Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create exportable summary data\n",
    "summary_export = {\n",
    "    'metadata': {\n",
    "        'generated_date': current_date,\n",
    "        'total_responses': len(df),\n",
    "        'total_fields': len(df.columns),\n",
    "        'data_range': f\"{df['user_information.date'].min()} to {df['user_information.date'].max()}\"\n",
    "    },\n",
    "    'overall_metrics': metrics_df.to_dict('records'),\n",
    "    'section_metrics': section_metrics_df.to_dict('records'),\n",
    "    'key_scores': {\n",
    "        'average_mop_effectiveness': avg_mop if 'avg_mop' in locals() else None,\n",
    "        'sus_score': eval_metrics.get('SUS Score'),\n",
    "        'nps_score': eval_metrics.get('NPS', 50) - 50 if 'NPS' in eval_metrics else None,\n",
    "        'recommendation_rate': eval_metrics.get('Would Recommend'),\n",
    "        'average_completion_rate': df.notna().sum().sum() / (len(df) * len(df.columns)) * 100\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save summary\n",
    "import json\n",
    "with open('jcc2_executive_summary.json', 'w') as f:\n",
    "    json.dump(summary_export, f, indent=2)\n",
    "\n",
    "print(\"\\n=== SUMMARY DATA EXPORTED ===\")\n",
    "print(\"Summary data has been exported to: jcc2_executive_summary.json\")\n",
    "print(\"\\nThis executive summary notebook provides a compressed view of:\")\n",
    "print(\"- Overall data quality and response metrics\")\n",
    "print(\"- Key metrics for each major section\")\n",
    "print(\"- Critical insights across 4 key dimensions\")\n",
    "print(\"- Executive summary with actionable recommendations\")\n",
    "print(\"- Exportable summary data for reporting\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
